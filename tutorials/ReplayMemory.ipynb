{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiences Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Simple Explanation\n",
    "\n",
    "In Reinforcement Learning, agents are trained mainly based on information $(s_t, a_t, s_{t+1}, r_t)$, where $r_t$ is the reward received at time $t$, and $s_{t+1}$ is the state of setting appears at time ${t+1}$ when executd action $a_t$ at state $s_t$.\n",
    "\n",
    "$(s_t, a_t, s_{t+1}, r_t)$ is called **Experience**, because this kind of information can be got only when the agents taking actions in the task environment, which is also called **Sampling** in RL.\n",
    "\n",
    "- **Goal of Reinforcement Learning**: learn an optimal or near optimal policy ($\\pi : S \\to A$, a rule about choosing which action $a$ when encounter the state $s$), that maximize the total reward $\\sum_{t}^{T}r(s_t, a_t)$ it can receive, from enough experience got through sampling.\n",
    "\n",
    "Utterly rational (extreme optimal) is possible when we have extreme computational power, this is the same for both human and machine. However, it's impossible to have such power, because problems can have state space being infinite complex, which can be easily understand when you think about your real-life experiences of nearly never encounter a specific state twice. What's more, it's hard to have a omniscient perspective, and mostly the environment are partial observed.\n",
    "\n",
    "So, researches have been trying to improve RL algorithms and architectures on **Efficiency** (learning quicker with less sampling), **Stability** (guarantee convergence when training), and **Scale** (handling settings with high states and action space).  \n",
    "\n",
    "One way to do the optimization is through action-value $Q(s,a)$, which is the estimate of total future rewards the can be received if takes action $a$ at state $s$. Such value can be used to decide policy, for example, the simplest way is greedly choosing the action with highest $Q(s,a)$. In order to solve the problem carried by *high-scale*, we need to do generalization on $s$ and $a$ via [Value Function Approximation](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf). For **Deep Reinforcement Learning**, the meaning is using **deep learning model** as the function approximator.\n",
    "\n",
    "**Experience replay** uses a replay memory buffer $\\mathcal{D}$ to store those transition experiences $(s_t, a_t, s_{t+1}, r_t)$, then  randomly extract batches from the memory to train Neural Network, playing a role of modeling transmition dynamics in model-free Reinforcement Learning. And it has amazing function on improve *efficiency* and *stability*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages\n",
    "\n",
    "- Samples are no longer correlated, this helps improve the networks convergence.\n",
    "- Like what batch gradient decident can do for training neural network, it more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works?\n",
    "\n",
    "[DQN](https://www.nature.com/articles/nature14236) uses experience replay and a 'fixed' Q-learning targets.\n",
    "- Take action $a_t$ according to $\\epsilon$-greedy policy\n",
    "- Store transition $(s_t, a_t, s_{t+1}, r_t)$ in replay memory $\\mathcal{D}$\n",
    "- Sample random mini-batch of transitions $(s_t, a_t, s_{t+1}, r_t)$ from $\\mathcal{D}$\n",
    "- Compute Q-learning targets w.r.t. old, fixed parameters $w^-$\n",
    "- Optimise MSE between Q-network and Q-learning targets\n",
    "$$\\mathcal{L}_i(w_i) = \\mathbb{E}_{s,a,r,s'\\thicksim\\mathcal{D}_i}[(r+\\gamma \\max_{a'}Q(s',a';w_i^-)-Q(s,a;w_i))]^2$$\n",
    "- Using variant of stochastic gradient descent\n",
    "\n",
    "After some iterations, we will update the parameters $w^-$ of the fixed target network with $w$. This can help avoid the time waste on updating so frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations \n",
    "\n",
    "Researchers have proposed various version of memory replay technology to improve the efficiency of training value function approximator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pythonrl 3",
   "language": "python",
   "name": "rlpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
