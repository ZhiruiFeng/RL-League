{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "\n",
    "The original paper published on Nature can be found [here](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). And there are two exercellent learning materials for it can be found from [@Hvass-Labs](https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/16_Reinforcement_Learning.ipynb) and [@dennybritz](https://github.com/ZhiruiFeng/reinforcement-learning/tree/master/DQN). \n",
    "\n",
    "As metioned by its creator, DeepMind, its meaning lies on:\n",
    "> This work represents the first demonstration of a general-purpose agent that is able to continually adapt its behavior without any human intervention, a major technical step forward in the quest for general AI.\n",
    "\n",
    "<img src=\"./resource/images/dqn-1.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "The above figure is borrowed from a [Nature New&View article](https://www.nature.com/articles/518486a.epdf?referrer_access_token=rJi2LNPaO_wh7LCXE8J0gNRgN0jAjWel9jnR3ZoTv0M4DtkukdMkIcR-UVrz0pNp311MkppKL7NysMmwcju-Md7bwkauG8hqmn4c75o_6pA%3D&tracking_referrer=http%3A%2F%2Fwww.nature.com%2Fnews%2Fnewsandviews) which talks about DQN.\n",
    "\n",
    "Till now, it has become the scaffold of Deep Reinforcement Learning, and offers the core architecture for further improvements, which makes it's really worthwile for everyone who want to work on RL to have a deep and thoroughly look at this model.\n",
    "\n",
    "And within project **RL-League**, you will find nearlly all other complex models are extensions of this one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is DQN?\n",
    "\n",
    "We can see DQN as Q-learning algorithm with a Neural Network plays the role of action-value function approximator, which enables the agent learn to choose actions from high-dimentional raw-image input. In Q-learning, $Q^*(s,a)$ represents the accumulated future reward, $Q^*$, if in state $s$ the system first performs action $a$, and subsequently follows an optimal policy. For small state space, tabular RL uses a look-up table to update $Q(s,a)$, which fails when the state space become very large. So here in DQN, the system tries to approximate $Q^*$ by using an artificial neural network - a function approximator to do generalization on states.\n",
    "\n",
    "The loss function for the function approximator - deep q-network is: $$\\mathcal{L}_i(w_i) = \\mathbb{E}_{s,a,r,s'\\thicksim\\mathcal{D}_i}[(r+\\gamma \\max_{a'}Q(s',a';w_i^-)-Q(s,a;w_i))]^2$$\n",
    "\n",
    "In this equation: \n",
    "- $i$ indicate the time; \n",
    "- $\\mathcal{D}_i$ means the replay buffer for [experience replay](https://medium.com/@Fihezro/experience-replay-deep-reinforcement-learning-ef02b8a1383); \n",
    "- $s'$ is the next state after taking action $a$ at state $s$, note that in dynamic environment, the $s'$ may not be determined by $s$ and $a$; \n",
    "- $w_i^-$ is the parameter of network for Q-learning target, which will not be updated frequently, but when updated, it will be set as $w_i$, the parameter of Q-Network;\n",
    "- $\\gamma$ is the discount factor;\n",
    "- $r+\\gamma \\max_{a'}Q(s',a';w_i^-)$ means the 'ground truth', assume the optimalization of later actions;\n",
    "- $Q(s,a;w_i)$ is the evaluation result of Q-network, and the $w_i$ is the parameter updated through gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of this notebook\n",
    "\n",
    "As we mentioned in the introduction of this project, we are going to use a modular view to see these RL models, you can find the reason [here](http://fzruniverse.life/2018/03/24/Modular-Architecture-for-Implementing-RL-Agent/).\n",
    "\n",
    "<img src=\"./resource/images/dqn-2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Modules are implemented in `/agents/modules/`, here we will just invoke these class and combine them into DQN agent. And an encapsulation of DQN agent can be find under `/agents/`. You can use it to play on `/field/`, where those agents solve problems in virtual game simulators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if \"../agents/modules\" not in sys.path:\n",
    "    sys.path.append(\"../agents/modules\")\n",
    "from accessory import LogQValues, LogReward, \n",
    "from sensedisposer import MotionTracer\n",
    "from controller import EpsilonGreedy\n",
    "from approximator import NeuralNetwork\n",
    "from replaymemory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, env_name, training, render=False, use_logging=True):\n",
    "        \n",
    "        self.env = gym.make(env.name)\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.training = training\n",
    "        self.render = render\n",
    "        self.use_logging = use_logging\n",
    "        \n",
    "        # Log-classes are used for restored the logs\n",
    "        if self.use_logging and self.training:\n",
    "            self.log_q_values = LogQValues()\n",
    "            self.log_reward = LogReward()\n",
    "        else:\n",
    "            self.log_q_values = None\n",
    "            self.log_reward = None\n",
    "        \n",
    "        self.action_names = self.env.unwrapped.get_action_meanings()\n",
    "        \n",
    "        # The probability of choosing a random action should decrease durin the training time.\n",
    "        self.epsilon_greedy = EpsilonGreedy(start_value=1.0,\n",
    "                                            end_value=0.1,\n",
    "                                            num_iterations=1e6,\n",
    "                                            num_actions=self.num_actions,\n",
    "                                            epsilon_testing=0.01)\n",
    "        \n",
    "        # Setting of hyper-parameters \n",
    "        if self.training:\n",
    "            self.learning_rate_control = LinearControlSignal(start_value=1e-3,\n",
    "                                                             end_value=1e-5,\n",
    "                                                             num_iterations=5e6)\n",
    "            self.loss_limit_control = LinearControlSignal(start_value=0.1,\n",
    "                                                          end_value=0.015,\n",
    "                                                          num_iterations=5e6)\n",
    "            self.max_epochs_control = LinearControlSignal(start_value=5.0,\n",
    "                                                          end_value=10.0,\n",
    "                                                          num_iterations=5e6)\n",
    "            self.replay_fraction = LinearControlSignal(start_value=0.1,\n",
    "                                                       end_value=1.0,\n",
    "                                                       num_iterations=5e6)\n",
    "        else:\n",
    "            self.learning_rate_control = None\n",
    "            self.loss_limit_control = None\n",
    "            self.max_epochs_control = None\n",
    "            self.replay_fraction = None\n",
    "        \n",
    "        # Setting of replay memory\n",
    "        if self.training:\n",
    "            self.replay_memory = ReplayMemory(size=200000, num_actions=self.num_actions)\n",
    "        else:\n",
    "            self.replay_memory = None\n",
    "        \n",
    "        # Create the Neural Network used for estimating Q-values.\n",
    "        # TODO: implementation of the approximator.\n",
    "        self.model = NeuralNetwork(num_actions=self.num_actions,\n",
    "                                   replay_memory=self.replay_memory)\n",
    "        # Log of the rewards\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "    def reset_episode_reward(self):\n",
    "        \"\"\"Reset the log of episode-rewards.\"\"\"\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def get_action_name(self, action):\n",
    "        \"\"\"Return the name of an action.\"\"\"\n",
    "        return self.action_names[action]\n",
    "    \n",
    "    def get_lives(self):\n",
    "        \"\"\"Get the number of lives the agent has in the game-environment.\"\"\"\n",
    "        return self.env.unwrapped.ale.lives()\n",
    "    \n",
    "    def run(self, num_episodes=None):\n",
    "        \"\"\"\n",
    "        Run the game-environment and use the Neural Network to decide\n",
    "        which actions to take in each step through Q-value estimates.\n",
    "        \n",
    "        :param num_episodes: \n",
    "            Number of episodes to process in the game-environment.\n",
    "            If None then continue forever. This is useful during training\n",
    "            where you might want to stop the training using Ctrl-C instead.\n",
    "        \"\"\"\n",
    "        end_episode = True\n",
    "        count_states = self.model.get_count_states()\n",
    "        count_episodes = self.model.get_count_episodes()\n",
    "        if num_episodes is None:\n",
    "            num_episodes = float('inf')\n",
    "        else:\n",
    "            num_episodes += count_episodes\n",
    "        \n",
    "        while count_episodes <= num_episodes:\n",
    "            if end_episode:\n",
    "                img = self.env.reset()\n",
    "                motion_tracer = MotionTracer(img)\n",
    "                reward_episode = 0.0\n",
    "                count_episodes = self.model.increase_count_episodes()\n",
    "                num_lives = self.get_lives()\n",
    "            state = motion_tracer.get_state()\n",
    "            q_values = self.model.get_q_values(states=[state])[0]\n",
    "            action, epsilon = self.epsilon_greedy.get_action(q_values=q_values,\n",
    "                                                             iteration=count_states,\n",
    "                                                             training=self.training)\n",
    "            img, reward, end_episode, info = self.env.step(action=action)\n",
    "            motion_tracer.process(image=img)\n",
    "            reward_episode += reward\n",
    "            num_lives_new = self.get_lives()\n",
    "            end_life = (num_lives_new < num_lives)\n",
    "            num_lives = num_lives_new\n",
    "            count_states = self.model.increase_count_states()\n",
    "            \n",
    "            if not self.training and self.render:\n",
    "                # Help you see the performance when test.\n",
    "                self.env.render()\n",
    "                time.sleep(0.01)\n",
    "            if self.training:\n",
    "                self.replay_memory.add(state=state,\n",
    "                                       q_values=q_values,\n",
    "                                       action=action,\n",
    "                                       reward=reward,\n",
    "                                       end_life=end_life,\n",
    "                                       end_episode=end_episode)\n",
    "                use_fraction = self.replay_fraction.get_value(iteration=count_states)\n",
    "                if self.replay_memory.is_full() or self.replay_memory.used_fraction() > use_fraction:\n",
    "                    self.replay_memory.update_all_q_values()\n",
    "                    if self.use_logging:\n",
    "                        self.log_q_values.write(count_episodes=count_episodes,\n",
    "                                                count_states=count_states,\n",
    "                                                q_values=self.replay_memory.q_values)\n",
    "                    learning_rate = self.learning_rate_control.get_value(iteration=count_states)\n",
    "                    loss_limit = self.loss_limit_control.get_value(iteration=count_states)\n",
    "                    max_epochs = self.max_epochs_control.get_value(iteration=count_states)\n",
    "                    self.model.optimize(learning_rate=learning_rate,\n",
    "                                        loss_limit=loss_limit,\n",
    "                                        max_epochs=max_epochs)\n",
    "                    self.model.save_checkpoint(count_states)\n",
    "                    self.replay_memory.reset()\n",
    "                \n",
    "            if end_episode:\n",
    "                self.episode_rewards.append(reward_episode)\n",
    "            \n",
    "            if len(self.episode_rewards) == 0:\n",
    "                reward_mean = 0.0\n",
    "            else:\n",
    "                reward_mean = np.mean(self.episode_reward[-30:])\n",
    "            \n",
    "            if self.training and end_episode:\n",
    "                # Log reward to file.\n",
    "                if self.use_logging:\n",
    "                    self.log_reward.write(count_episodes=count_episodes,\n",
    "                                          count_states=count_states,\n",
    "                                          reward_episode=reward_episode,\n",
    "                                          reward_mean=reward_mean)\n",
    "                # Print reward to screen.\n",
    "                msg = \"{0:4}:{1}\\t Epsilon: {2:4.2f}\\t Reward: {3:.1f}\\t Episode Mean: {4:.1f}\"\n",
    "                print(msg.format(count_episodes, count_states, epsilon,\n",
    "                                 reward_episode, reward_mean))\n",
    "            elif not self.training and (reward != 0.0 or end_life or end_episode):\n",
    "                msg = \"{0:4}:{1}\\tQ-min: {2:5.3f}\\tQ-max: {3:5.3f}\\tLives: {4}\\tReward: {5:.1f}\\tEpisode Mean: {6:.1f}\"\n",
    "                print(msg.format(count_episodes, count_states, np.min(q_values),\n",
    "                                 np.max(q_values), num_lives, reward_episode, reward_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "# Get the arguments.\n",
    "env_name = 'Breakout-v0'\n",
    "training = True\n",
    "render = False\n",
    "num_episodes = 100\n",
    "checkpoint_base_dir = \"./experiment/checkpoint\"\n",
    "\n",
    "# TODO solve the match of path.\n",
    "\n",
    "agent = Agent(env_name=env_name,\n",
    "              training=training,\n",
    "              render=render)\n",
    "agent.run(num_episodes=num_episodes)\n",
    "\n",
    "rewards = agent.episode_rewards\n",
    "print()  # Newline.\n",
    "print(\"Rewards for {0} episodes:\".format(len(rewards)))\n",
    "print(\"- Min:   \", np.min(rewards))\n",
    "print(\"- Mean:  \", np.mean(rewards))\n",
    "print(\"- Max:   \", np.max(rewards))\n",
    "print(\"- Stdev: \", np.std(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pythonrl 3",
   "language": "python",
   "name": "rlpython3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
